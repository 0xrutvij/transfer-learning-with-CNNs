{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Found 12847 files belonging to 13 classes.\n",
      "Using 10278 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 23:32:49.691810: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-11 23:32:49.692429: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-11 23:32:49.692695: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: AMD Radeon Pro 560X\n",
      "Found 12847 files belonging to 13 classes.\n",
      "Using 2569 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filename:  proj2.ipynb\n",
    "Date:      04/12/2022\n",
    "Author:    Rutvij Shah\n",
    "Email:     rutvij.shah@utdallas.edu\n",
    "Course:    CS6384 Spring 2022\n",
    "Version:   1.0\n",
    "Copyright: 2022, All Rights Reserved\n",
    "\n",
    "Description: Code to train the transfer learning model.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from random import choice\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "data_dir = Path(\"/Users/rutvijshah/CodeBase/MachineLearning/TransferLearningCNNs/flowers\")\n",
    "\n",
    "image_count = len(list(data_dir.rglob(\"*.jpg\")))\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 160\n",
    "img_width = 160\n",
    "img_size = (img_width, img_height)\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take(val_batches // 5)\n",
    "val_ds = val_ds.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 160, 160, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 160, 160, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 2048)              21802784  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                131136    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                845       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,934,765\n",
      "Trainable params: 131,981\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds, val_ds, test_ds = map(lambda x: x.prefetch(buffer_size=AUTOTUNE), [train_ds, val_ds, test_ds])\n",
    "\n",
    "IMG_SHAPE = img_size + (3,)\n",
    "\n",
    "base_model = hub.KerasLayer(\"https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5\", trainable=False)\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=[img_width, img_height, 3])\n",
    "x = tf.keras.applications.inception_v3.preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "outputs = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 23:33:09.944075: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 17s 186ms/step - loss: 3.8384 - accuracy: 0.0690\n",
      "initial loss: 3.84\n",
      "initial accuracy: 0.07\n"
     ]
    }
   ],
   "source": [
    "base_learning_rate = 0.001\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(val_ds)\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65/65 [==============================] - 14s 210ms/step - loss: 0.5765 - accuracy: 0.8610 - val_loss: 0.4603 - val_accuracy: 0.8809\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 13s 200ms/step - loss: 0.4002 - accuracy: 0.8736 - val_loss: 0.2689 - val_accuracy: 0.9219\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 13s 200ms/step - loss: 0.3750 - accuracy: 0.8785 - val_loss: 0.3855 - val_accuracy: 0.9004\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 13s 200ms/step - loss: 0.2914 - accuracy: 0.9067 - val_loss: 0.3143 - val_accuracy: 0.9219\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 13s 200ms/step - loss: 0.2554 - accuracy: 0.9217 - val_loss: 0.3702 - val_accuracy: 0.8926\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 13s 204ms/step - loss: 0.2161 - accuracy: 0.9242 - val_loss: 0.2590 - val_accuracy: 0.9316\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 17s 261ms/step - loss: 0.1769 - accuracy: 0.9266 - val_loss: 0.2568 - val_accuracy: 0.9316\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 21s 327ms/step - loss: 0.1769 - accuracy: 0.9387 - val_loss: 0.2529 - val_accuracy: 0.9355\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 19s 292ms/step - loss: 0.1654 - accuracy: 0.9480 - val_loss: 0.2624 - val_accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 15s 220ms/step - loss: 0.1289 - accuracy: 0.9558 - val_loss: 0.2121 - val_accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.2477 - accuracy: 0.9434\n",
      "{'loss': 0.2476959079504013, 'accuracy': 0.943359375}\n",
      "INFO:tensorflow:Assets written to: models/inception_inaturalist_131k_20ep_valCross/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/inception_inaturalist_131k_20ep_valCross/assets\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_ds)\n",
    "print(dict(zip(model.metrics_names, result)))\n",
    "\n",
    "model.save(\"models/inception_inaturalist_131k_20ep_valCross\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 160, 160, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 160, 160, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 2048)              21802784  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                131136    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                845       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,934,765\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 166,413\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/inception_inaturalist_131k_20ep_valCross\")\n",
    "\n",
    "model.layers[3].trainable = True\n",
    "\n",
    "model.layers[-1].trainable = False\n",
    "model.layers[-2].trainable = False\n",
    "model.layers[-3].trainable = False\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "65/65 [==============================] - 36s 542ms/step - loss: 0.1459 - accuracy: 0.9533 - val_loss: 0.2337 - val_accuracy: 0.9531\n",
      "Epoch 2/2\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 0.1144 - accuracy: 0.9708 - val_loss: 0.1811 - val_accuracy: 0.9570\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x178d1dd30>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate\n",
      "322/322 [==============================] - 71s 221ms/step - loss: 0.0420 - accuracy: 0.9859\n",
      "{'loss': 0.041951991617679596, 'accuracy': 0.9858922362327576}\n",
      "INFO:tensorflow:Assets written to: models/inception_inaturalist_131k_20ep_valCross_fineTuned3ep_frozenTop_valCross/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/inception_inaturalist_131k_20ep_valCross_fineTuned3ep_frozenTop_valCross/assets\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_ds)\n",
    "print(dict(zip(model.metrics_names, result)))\n",
    "\n",
    "model.save(\"models/inception_inaturalist_131k_20ep_valCross_fineTuned3ep_frozenTop_valCross\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'astilbe',\n 1: 'bellflower',\n 2: 'black-eyed susan',\n 3: 'calendula',\n 4: 'california poppy',\n 5: 'carnation',\n 6: 'common daisy',\n 7: 'coreopsis',\n 8: 'dandelion',\n 9: 'iris',\n 10: 'rose',\n 11: 'sunflower',\n 12: 'tulip'}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i: name for i, name in enumerate(class_names)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def predict_from_url(name, url):\n",
    "    img_path = tf.keras.utils.get_file(name, origin=url)\n",
    "\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_path, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "            .format(class_names[np.argmax(predictions)], 100 * np.max(predictions))\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 00:49:01.625470: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to california poppy with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "url_cal_poppy = \"http://drystonegarden.com/wp-content/uploads/2010/03/calpoppymarch14.jpg\"\n",
    "\n",
    "predict_from_url(\"cp1\", url_cal_poppy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to california poppy with a 99.99 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "url_cal_poppy_2 = \"https://cdn.shopify.com/s/files/1/0011/2341/8172/products/FL3086-1_1024x1024.jpg?v=1523415127\"\n",
    "\n",
    "predict_from_url(\"cp2\", url_cal_poppy_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to rose with a 48.76 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"astilbe1\",\n",
    "    \"https://images-na.ssl-images-amazon.com/images/I/91NXQ6lqzAL._AC_SL1500_.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to astilbe with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"astilbe2\",\n",
    "    \"https://img.crocdn.co.uk/images/products2/pl/00/00/00/25/pl0000002573.jpg?width=940&height=940\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to astilbe with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"astilbe3\",\n",
    "    \"https://cdn.shopify.com/s/files/1/1419/7120/products/Astilbe_Bridal_Veil_2_.DeG.jpg_1d35e2be-3d36-453d-9058-5c84a6947cdf_800x.JPG?v=1536082862\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to bellflower with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"bell1\",\n",
    "    \"https://www.nature-and-garden.com/wp-content/uploads/2017/10/bellflower-care.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to bellflower with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"bell2\",\n",
    "    \"http://www.authenticwisconsin.com/images/creeping_bellflower_14_6_26_20.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to black-eyed susan with a 99.49 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"bes1\",\n",
    "    \"https://cdn.britannica.com/64/197864-050-62DC4816/Black-eyed-Susan-North-America.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://i.ebayimg.com/images/i/331205855715-0-1/s-l1000.jpg\n",
      "188416/188096 [==============================] - 0s 0us/step\n",
      "196608/188096 [===============================] - 0s 0us/step\n",
      "This image most likely belongs to calendula with a 98.08 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"cal1\",\n",
    "    \"https://i.ebayimg.com/images/i/331205855715-0-1/s-l1000.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.morroccomethod.com/blog/wp-content/uploads/2013/10/Calendula_officinalis_001.jpg\n",
      "2072576/2071620 [==============================] - 2s 1us/step\n",
      "2080768/2071620 [==============================] - 2s 1us/step\n",
      "This image most likely belongs to calendula with a 67.95 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"cal2\",\n",
    "    \"https://www.morroccomethod.com/blog/wp-content/uploads/2013/10/Calendula_officinalis_001.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://i.pinimg.com/originals/a6/5e/94/a65e9427f77cd0e730744db316fa4359.jpg\n",
      "262144/260706 [==============================] - 0s 0us/step\n",
      "270336/260706 [===============================] - 0s 0us/step\n",
      "This image most likely belongs to carnation with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"carn1\",\n",
    "    \"https://i.pinimg.com/originals/a6/5e/94/a65e9427f77cd0e730744db316fa4359.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.outsidepride.com/images/products/detail/gardenflower/carnationpicoteemix.jpg\n",
      "212992/208116 [==============================] - 0s 1us/step\n",
      "221184/208116 [===============================] - 0s 1us/step\n",
      "This image most likely belongs to carnation with a 99.94 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"carn2\",\n",
    "    \"https://www.outsidepride.com/images/products/detail/gardenflower/carnationpicoteemix.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://nurserynisarga.in/wp-content/uploads/2020/03/IMG_20200322_081910.jpg\n",
      "368640/365388 [==============================] - 2s 6us/step\n",
      "376832/365388 [==============================] - 2s 6us/step\n",
      "This image most likely belongs to carnation with a 91.64 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"carn3\",\n",
    "    \"https://nurserynisarga.in/wp-content/uploads/2020/03/IMG_20200322_081910.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://images.fineartamerica.com/images-medium-large-5/red-carnation-john-koscinski.jpg\n",
      "65536/60292 [================================] - 0s 0us/step\n",
      "73728/60292 [====================================] - 0s 0us/step\n",
      "This image most likely belongs to carnation with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"carn4\",\n",
    "    \"https://images.fineartamerica.com/images-medium-large-5/red-carnation-john-koscinski.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/d1c4ab42-891a-4de6-afed-4b3ae0898def/d62lsri-e55c7faf-8b99-4eaa-b1d0-0995e5c936da.jpg/v1/fill/w_900,h_675,q_75,strp/common_daisy_by_graylynx-d62lsri.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwic3ViIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl0sIm9iaiI6W1t7InBhdGgiOiIvZi9kMWM0YWI0Mi04OTFhLTRkZTYtYWZlZC00YjNhZTA4OThkZWYvZDYybHNyaS1lNTVjN2ZhZi04Yjk5LTRlYWEtYjFkMC0wOTk1ZTVjOTM2ZGEuanBnIiwid2lkdGgiOiI8PTkwMCIsImhlaWdodCI6Ijw9Njc1In1dXX0.6baZKstgOqapgmfSwQQGaaI6PWxzEGh61hAk21aLDqU\n",
      "155648/152307 [==============================] - 0s 0us/step\n",
      "163840/152307 [================================] - 0s 0us/step\n",
      "This image most likely belongs to common daisy with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"comDaisy1\",\n",
    "    \"https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/d1c4ab42-891a-4de6-afed-4b3ae0898def/d62lsri-e55c7faf-8b99-4eaa-b1d0-0995e5c936da.jpg/v1/fill/w_900,h_675,q_75,strp/common_daisy_by_graylynx-d62lsri.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwic3ViIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl0sIm9iaiI6W1t7InBhdGgiOiIvZi9kMWM0YWI0Mi04OTFhLTRkZTYtYWZlZC00YjNhZTA4OThkZWYvZDYybHNyaS1lNTVjN2ZhZi04Yjk5LTRlYWEtYjFkMC0wOTk1ZTVjOTM2ZGEuanBnIiwid2lkdGgiOiI8PTkwMCIsImhlaWdodCI6Ijw9Njc1In1dXX0.6baZKstgOqapgmfSwQQGaaI6PWxzEGh61hAk21aLDqU\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://media.istockphoto.com/photos/blooming-common-daisies-in-grass-picture-id518153683\n",
      "368640/363476 [==============================] - 0s 0us/step\n",
      "376832/363476 [===============================] - 0s 0us/step\n",
      "This image most likely belongs to common daisy with a 99.98 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"comDaisy2\",\n",
    "    \"https://media.istockphoto.com/photos/blooming-common-daisies-in-grass-picture-id518153683\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/common-daisy-3-arlane-crump.jpg\n",
      "122880/117102 [===============================] - 0s 0us/step\n",
      "131072/117102 [=================================] - 0s 0us/step\n",
      "This image most likely belongs to common daisy with a 98.52 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"comDaisy3\",\n",
    "    \"https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/common-daisy-3-arlane-crump.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.nature-and-garden.com/wp-content/uploads/2018/06/coreopsis-care.jpg\n",
      "114688/113103 [==============================] - 0s 2us/step\n",
      "122880/113103 [================================] - 0s 2us/step\n",
      "This image most likely belongs to coreopsis with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"coreop1\",\n",
    "    \"https://www.nature-and-garden.com/wp-content/uploads/2018/06/coreopsis-care.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://1.bp.blogspot.com/-vLga_VeM12E/WP_vdWemU6I/AAAAAAAAHhA/niwyFcC0aV0ZG1GW6G8Wq__1AKsut8bSgCEw/s1600/Coreopsis%2Bauriculata%2B005a.jpg\n",
      "172032/167544 [==============================] - 0s 0us/step\n",
      "180224/167544 [================================] - 0s 0us/step\n",
      "This image most likely belongs to coreopsis with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"coreop2\",\n",
    "    \"https://1.bp.blogspot.com/-vLga_VeM12E/WP_vdWemU6I/AAAAAAAAHhA/niwyFcC0aV0ZG1GW6G8Wq__1AKsut8bSgCEw/s1600/Coreopsis%2Bauriculata%2B005a.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://cdn11.bigcommerce.com/s-ih8o56kgor/images/stencil/1280x1280/products/3083/7832/C.-tinctoria-6.25.14-003__51470.1593615550.png?c=2\n",
      " 737280/Unknown - 0s 0us/stepThis image most likely belongs to coreopsis with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"coreop3\",\n",
    "    \"https://cdn11.bigcommerce.com/s-ih8o56kgor/images/stencil/1280x1280/products/3083/7832/C.-tinctoria-6.25.14-003__51470.1593615550.png?c=2\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.growjoy.com/store/pc/catalog/limerock_dream_coreopsis_plant_1104_detail.jpg\n",
      "204800/202947 [==============================] - 0s 0us/step\n",
      "212992/202947 [===============================] - 0s 0us/step\n",
      "This image most likely belongs to coreopsis with a 99.89 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"coreops4\",\n",
    "    \"https://www.growjoy.com/store/pc/catalog/limerock_dream_coreopsis_plant_1104_detail.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Iris_germanica_(Purple_bearded_Iris)%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg/1200px-Iris_germanica_(Purple_bearded_Iris)%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg\n",
      "245760/239117 [==============================] - 0s 0us/step\n",
      "253952/239117 [===============================] - 0s 0us/step\n",
      "This image most likely belongs to iris with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"iris1\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Iris_germanica_(Purple_bearded_Iris)%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg/1200px-Iris_germanica_(Purple_bearded_Iris)%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://cdn.shopify.com/s/files/1/1419/7120/products/1V8A8594-2.jpg?v=1571439555\n",
      "876544/869952 [==============================] - 0s 0us/step\n",
      "884736/869952 [==============================] - 0s 0us/step\n",
      "This image most likely belongs to iris with a 92.80 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"iris2\",\n",
    "    \"https://cdn.shopify.com/s/files/1/1419/7120/products/1V8A8594-2.jpg?v=1571439555\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://garden.org/pics/2018-04-29/janwax/a781ce.jpg\n",
      "  16384/Unknown - 0s 1us/stepThis image most likely belongs to iris with a 98.59 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"iris3\",\n",
    "    \"https://garden.org/pics/2018-04-29/janwax/a781ce.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.thespruce.com/thmb/pYi1QejdwRY7hDQ9jzw8OLlLxbw=/5750x3827/filters:fill(auto,1)/how-to-grow-dandelion-greens-2539624-02-c810bc0c96e24b6bba96818173acf8fe.jpg\n",
      "1359872/1356118 [==============================] - 0s 0us/step\n",
      "1368064/1356118 [==============================] - 0s 0us/step\n",
      "This image most likely belongs to dandelion with a 99.65 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"dandelion1\",\n",
    "    \"https://www.thespruce.com/thmb/pYi1QejdwRY7hDQ9jzw8OLlLxbw=/5750x3827/filters:fill(auto,1)/how-to-grow-dandelion-greens-2539624-02-c810bc0c96e24b6bba96818173acf8fe.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/1-dandelion-seeds-bess-hamiti.jpg\n",
      "114688/109359 [===============================] - 0s 0us/step\n",
      "122880/109359 [=================================] - 0s 0us/step\n",
      "This image most likely belongs to dandelion with a 93.69 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"dande2\",\n",
    "    \"https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/1-dandelion-seeds-bess-hamiti.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.veganchoicefoods.com/image/cache/catalog/Dandelion-LEAF-Cut-ORGANIC-Loose-Herbal-TEA-Taraxacum-officinale25g850g/Dandelion-LEAF-Cut-ORGANIC-Loose-Herbal-TEA-Taraxacum-officinale25g850g-28305585-2-800x800.jpg\n",
      "221184/218795 [==============================] - 0s 1us/step\n",
      "229376/218795 [===============================] - 0s 1us/step\n",
      "This image most likely belongs to dandelion with a 99.97 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "predict_from_url(\n",
    "    \"dande3\",\n",
    "    \"https://www.veganchoicefoods.com/image/cache/catalog/Dandelion-LEAF-Cut-ORGANIC-Loose-Herbal-TEA-Taraxacum-officinale25g850g/Dandelion-LEAF-Cut-ORGANIC-Loose-Herbal-TEA-Taraxacum-officinale25g850g-28305585-2-800x800.jpg\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}